{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414642a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"/data/Nes/kaggle_requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c09c20",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/data/Nes/cache\" # controls all HF caches\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/data/Nes/cache\" # models, tokenizers, Hub files\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/data/Nes/cache\" # datasets/Arrow files\n",
    "\n",
    "print(\"HF_HUB_CACHE:\", os.getenv(\"HF_HUB_CACHE\"))\n",
    "print(\"HF_DATASETS_CACHE:\", os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "local_path = \"/data/Nes/model cache\"  # path ที่คุณอยากเก็บ model/tokenizer\n",
    "\n",
    "# โหลด tokenizer และ model แล้วเก็บไว้ใน path \"xxx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=local_path)  # โหลด tokenizer แล้วเก็บไว้ใน path ที่กำหนด\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",              # ให้ PyTorch auto เลือก dtype เช่น float16 ถ้ามี GPU\n",
    "    device_map=\"auto\",               # ให้ transformers กระจาย model อัตโนมัติลง GPU/CPU\n",
    "    cache_dir=local_path             # โหลด model แล้วเก็บไว้ใน path ที่กำหนด\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d51d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt analysis\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a senior critical-analysis assistant with deep expertise in economics, finance, and business reasoning.\n",
    "\n",
    "Your task is to **analyze and critique** the structure of a multiple-choice question in Thai or English by thinking through the following steps before giving a final judgment.\n",
    "\n",
    "Follow this Chain-of-Thought (CoT) reasoning:\n",
    "\n",
    "1. Identify the main economic, financial, or business concept being tested.\n",
    "2. Examine the clarity of the question's wording. Is it precise? Are there any ambiguities?\n",
    "3. Evaluate whether each answer choice meaningfully relates to the core concept.\n",
    "4. Check for flaws in logic, assumptions, or missing context.\n",
    "5. Assess whether the question is useful for testing understanding of a real, testable concept.\n",
    "6. Then write a **brief structured critique** based only on logic and clarity — not correctness.\n",
    "\n",
    "STRICT RULES:\n",
    "- Do NOT state which option is correct.\n",
    "- Do NOT confirm or mention the correct answer.\n",
    "- Focus solely on the structure, clarity, logic, and relevance of the question and choices.\n",
    "- Be concise and use professional tone.\n",
    "- Do NOT explain your reasoning steps in the output.\n",
    "\n",
    "FINAL OUTPUT FORMAT (strict):\n",
    "Return ONLY the following JSON structure — nothing else.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"analyse\": \"Your short critique here.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea311f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#หัวข้อ: Loop generate คำตอบจาก Qwen 3 8B ให้เร็วและกระชับ\n",
    "\n",
    "r = 0  # ตัวแปรนับรอบ\n",
    "ans = []  # รายการเก็บคำตอบทั้งหมด\n",
    "\n",
    "for x, i in df.iterrows():  # loop ทีละแถวจาก DataFrame\n",
    "    a = i['query']  # คำถาม\n",
    "\n",
    "    # รวม query และ concept ให้เป็นข้อความเดียวใน user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt เพื่อกำหนดพฤติกรรม\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\"}  # รวมคำถามกับ concept\n",
    "    ]\n",
    "\n",
    "    # สร้าง prompt template ที่ใช้กับ Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # ❌ ปิด thinking mode เพื่อความเร็ว\n",
    "    )\n",
    "\n",
    "    # แปลงเป็น input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ให้ model generate คำตอบ\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=200,  # ✅ จำกัดความยาว output ให้เร็วขึ้น\n",
    "        do_sample=False,  # ✅ ไม่สุ่ม ใช้ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ✅ ป้องกัน warning เรื่อง padding\n",
    "    )\n",
    "\n",
    "    # แยกเอาเฉพาะ token ที่ model สร้างใหม่ (ไม่รวม prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # แปลง token เป็นข้อความคำตอบ\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # แสดงผลและเก็บคำตอบ\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # เพิ่มรอบ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['analysis'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a458de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a831a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaa = pd.read_csv('analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt concept เเต่ตอนที่ใช้ infer เผลอลบออกเเล้วหาไม่เจอ\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial question analyst.\n",
    "\n",
    "Your job is to read a financial question and identify the **key concepts**, **required financial knowledge**, and **reasoning type** needed to answer it.\n",
    "\n",
    "Please respond with the following structure:\n",
    "\n",
    "1. **Main Topic:** (What is the question about? e.g., Stock movement, Macroeconomics, Earnings, Valuation...)\n",
    "2. **Financial Concepts Involved:** (List specific topics like interest rates, inflation, investor sentiment, EPS, risk premium, etc.)\n",
    "3. **Reasoning Type Needed:** (e.g., Cause-effect, Comparison, Trend inference, Risk evaluation)\n",
    "4. **Potential Data Needed:** (e.g., Interest rate trend, Company earnings report, Market sentiment index...)\n",
    "5. **Difficulty Level:** (Basic / Intermediate / Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "Example:\n",
    "\n",
    "**Question:**  \n",
    "\"The central bank raised interest rates while inflation is slowing. What will happen to stock prices?\"\n",
    "\n",
    "**Response:**  \n",
    "1. **Main Topic:** Stock price direction  \n",
    "2. **Financial Concepts Involved:** Interest rates, Inflation, Equity valuation  \n",
    "3. **Reasoning Type Needed:** Cause-effect and macroeconomic trend analysis  \n",
    "4. **Potential Data Needed:** Fed rate decision, CPI trend, bond yield curve  \n",
    "5. **Difficulty Level:** Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "Now analyze this question:\n",
    "\n",
    "**Question:** {<<< YOUR QUESTION HERE >>>}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#หัวข้อ: Loop generate คำตอบจาก Qwen 3 8B ให้เร็วและกระชับ\n",
    "\n",
    "r = 0  # ตัวแปรนับรอบ\n",
    "ans = []  # รายการเก็บคำตอบทั้งหมด\n",
    "\n",
    "for x, i in df.iterrows():  # loop ทีละแถวจาก DataFrame\n",
    "    a = i['query']  # คำถาม\n",
    "\n",
    "    # รวม query และ concept ให้เป็นข้อความเดียวใน user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt เพื่อกำหนดพฤติกรรม\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\"}  # รวมคำถามกับ concept\n",
    "    ]\n",
    "\n",
    "    # สร้าง prompt template ที่ใช้กับ Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # ❌ ปิด thinking mode เพื่อความเร็ว\n",
    "    )\n",
    "\n",
    "    # แปลงเป็น input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ให้ model generate คำตอบ\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=200,  # ✅ จำกัดความยาว output ให้เร็วขึ้น\n",
    "        do_sample=False,  # ✅ ไม่สุ่ม ใช้ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ✅ ป้องกัน warning เรื่อง padding\n",
    "    )\n",
    "\n",
    "    # แยกเอาเฉพาะ token ที่ model สร้างใหม่ (ไม่รวม prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # แปลง token เป็นข้อความคำตอบ\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # แสดงผลและเก็บคำตอบ\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # เพิ่มรอบ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('concept.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9798c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcc = pd.read_csv('/data/Nes/Finalhack/concept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852dffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaa = pd.read_csv('/data/Nes/Finalhack/analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"concept\"] = dfcc[\"concept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"analysis\"] = dfaa[\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87949c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt ตอนตอบ\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly analytical and domain-aware expert across the fields of Ethical Finance, Marketing, Port Management, Finance Management, Economics, Stock Movement, Fixed Income, Accounting, and Financial Reporting.\n",
    "\n",
    "Your role is to answer expert-level questions from these domains using internal step-by-step reasoning (Chain of Thought), but you must return ONLY the final answer without explanation.\n",
    "\n",
    "Instructions:\n",
    "Think step-by-step internally to arrive at the best answer (Chain of Thought reasoning is allowed internally).\n",
    "If the question provides multiple choices (e.g., A, B, C, D, E, F...), select and return ONLY one letter that corresponds to the best choice.\n",
    "If the question is about stock price, market movement, or share performance, return ONLY one of the following:\n",
    "\"Rise\" (if price or value is expected to increase)\n",
    "\"Fall\" (if price or value is expected to decrease)\n",
    "Do NOT display any explanations, reasoning, or justifications in your response.\n",
    "The final output must always follow this format exactly:\n",
    "Answer: <A/B/C/.../Rise/Fall>\n",
    "\n",
    "Remember: The answer must appear alone on the last line, prefixed by \"Answer: \".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#หัวข้อ: Loop generate คำตอบจาก Qwen 3 8B ให้เร็วและกระชับ\n",
    "\n",
    "r = 0  # ตัวแปรนับรอบ\n",
    "ans = []  # รายการเก็บคำตอบทั้งหมด\n",
    "\n",
    "for x, i in df.iterrows():  # loop ทีละแถวจาก DataFrame\n",
    "    a = i['query']  # คำถาม\n",
    "    b = i['concept']  # คอนเซปต์\n",
    "    c = i['analysis']  # คำวิเคราะห์\n",
    "\n",
    "    # รวม query และ concept ให้เป็นข้อความเดียวใน user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt เพื่อกำหนดพฤติกรรม\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\\nConcept: {b}\\nAnalysis: {c}\"}  # รวมคำถามกับ concept\n",
    "    ]\n",
    "\n",
    "    # สร้าง prompt template ที่ใช้กับ Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True  # ❌ ปิด thinking mode เพื่อความเร็ว เปิดเเล้วตอบดีขึ้น\n",
    "    )\n",
    "\n",
    "    # แปลงเป็น input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ให้ model generate คำตอบ\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=10000,  # ✅ จำกัดความยาว output ให้เร็วขึ้น ปิด thinking ข้างบนใช้ 100 \n",
    "        do_sample=False,  # ✅ ไม่สุ่ม ใช้ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ✅ ป้องกัน warning เรื่อง padding\n",
    "    )\n",
    "\n",
    "    # แยกเอาเฉพาะ token ที่ model สร้างใหม่ (ไม่รวม prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # แปลง token เป็นข้อความคำตอบ\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # แสดงผลและเก็บคำตอบ\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # เพิ่มรอบ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8619bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = pd.read_csv(\"/data/data/week10/submission (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb[\"answer\"]  = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"Answer: \", \"\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"ขึ้น\", \"Rise\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"ลง\", \"Fall\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(r\"\\.\\s*</think>\", \"\", regex=True).str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.to_csv(\"qwen38b+answer+concept.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc7aab",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36556e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = pd.read_csv(\"/data/Nes/Finalhack/qwen38b+answer+concept.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = pd.read_csv(\"/data/data/week10/processed_financial_analysis_flash2-5_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb058ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ch_preds = ch['answer']  # เป็น Series ของคำตอบจากโมเดล ch\n",
    "\n",
    "# ดึงคำตอบจาก sb ที่อยู่ใน column \"answer\"\n",
    "sb_truth = sb['answer']  # เป็น Series ของคำตอบจริง\n",
    "\n",
    "# คำนวณ accuracy ระหว่างคำตอบจาก ch และคำตอบจริง sb\n",
    "acc_ch = accuracy_score(sb_truth, ch_preds)  # คำนวณความแม่นยำ\n",
    "\n",
    "print(f\"Accuracy ของ ch: {acc_ch:.4f}\")  # แสดงผลเป็นทศนิยม 4 ตำแหน่ง\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # 🧼 แปลงทั้งสองคอลัมน์เป็น string และลบ NaN\n",
    "# ch_preds = ch['answer'].astype(str)      # แปลงเป็น string\n",
    "# sb_truth = ans['answer'].astype(str)     # แปลงเป็น string\n",
    "\n",
    "# # 🧹 กรองเฉพาะแถวที่ตรงกันและไม่มี missing\n",
    "# mask = (ch_preds.notna()) & (sb_truth.notna())\n",
    "# ch_preds_clean = ch_preds[mask]\n",
    "# sb_truth_clean = sb_truth[mask]\n",
    "\n",
    "# # ✅ คำนวณ accuracy\n",
    "# acc_ch = accuracy_score(sb_truth_clean, ch_preds_clean)\n",
    "\n",
    "# # 🖨️ แสดงผล\n",
    "# print(f\"Accuracy ของ ch: {acc_ch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.to_csv(\"qwen38b+answer+concept.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8b21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
