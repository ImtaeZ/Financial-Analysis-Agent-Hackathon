{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414642a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r \"/data/Nes/kaggle_requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c09c20",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/data/Nes/cache\" # controls all HF caches\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/data/Nes/cache\" # models, tokenizers, Hub files\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/data/Nes/cache\" # datasets/Arrow files\n",
    "\n",
    "print(\"HF_HUB_CACHE:\", os.getenv(\"HF_HUB_CACHE\"))\n",
    "print(\"HF_DATASETS_CACHE:\", os.getenv(\"HF_DATASETS_CACHE\"))\n",
    "print(\"HF_HOME:\", os.getenv(\"HF_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "local_path = \"/data/Nes/model cache\"  # path ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Å‡πá‡∏ö model/tokenizer\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡∏∞ model ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô path \"xxx\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=local_path)  # ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô path ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",              # ‡πÉ‡∏´‡πâ PyTorch auto ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡πÄ‡∏ä‡πà‡∏ô float16 ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU\n",
    "    device_map=\"auto\",               # ‡πÉ‡∏´‡πâ transformers ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ model ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏•‡∏á GPU/CPU\n",
    "    cache_dir=local_path             # ‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô path ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d51d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt analysis\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a senior critical-analysis assistant with deep expertise in economics, finance, and business reasoning.\n",
    "\n",
    "Your task is to **analyze and critique** the structure of a multiple-choice question in Thai or English by thinking through the following steps before giving a final judgment.\n",
    "\n",
    "Follow this Chain-of-Thought (CoT) reasoning:\n",
    "\n",
    "1. Identify the main economic, financial, or business concept being tested.\n",
    "2. Examine the clarity of the question's wording. Is it precise? Are there any ambiguities?\n",
    "3. Evaluate whether each answer choice meaningfully relates to the core concept.\n",
    "4. Check for flaws in logic, assumptions, or missing context.\n",
    "5. Assess whether the question is useful for testing understanding of a real, testable concept.\n",
    "6. Then write a **brief structured critique** based only on logic and clarity ‚Äî not correctness.\n",
    "\n",
    "STRICT RULES:\n",
    "- Do NOT state which option is correct.\n",
    "- Do NOT confirm or mention the correct answer.\n",
    "- Focus solely on the structure, clarity, logic, and relevance of the question and choices.\n",
    "- Be concise and use professional tone.\n",
    "- Do NOT explain your reasoning steps in the output.\n",
    "\n",
    "FINAL OUTPUT FORMAT (strict):\n",
    "Return ONLY the following JSON structure ‚Äî nothing else.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"analyse\": \"Your short critique here.\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea311f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: Loop generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Qwen 3 8B ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö\n",
    "\n",
    "r = 0  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏±‡∏ö‡∏£‡∏≠‡∏ö\n",
    "ans = []  # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "for x, i in df.iterrows():  # loop ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å DataFrame\n",
    "    a = i['query']  # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
    "\n",
    "    # ‡∏£‡∏ß‡∏° query ‡πÅ‡∏•‡∏∞ concept ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\"}  # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏±‡∏ö concept\n",
    "    ]\n",
    "\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt template ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # ‚ùå ‡∏õ‡∏¥‡∏î thinking mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ‡πÉ‡∏´‡πâ model generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=200,  # ‚úÖ ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "        do_sample=False,  # ‚úÖ ‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏° ‡πÉ‡∏ä‡πâ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô warning ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á padding\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ token ‡∏ó‡∏µ‡πà model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á token ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['analysis'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a458de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a831a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaa = pd.read_csv('analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt concept ‡πÄ‡πÄ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ infer ‡πÄ‡∏ú‡∏•‡∏≠‡∏•‡∏ö‡∏≠‡∏≠‡∏Å‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial question analyst.\n",
    "\n",
    "Your job is to read a financial question and identify the **key concepts**, **required financial knowledge**, and **reasoning type** needed to answer it.\n",
    "\n",
    "Please respond with the following structure:\n",
    "\n",
    "1. **Main Topic:** (What is the question about? e.g., Stock movement, Macroeconomics, Earnings, Valuation...)\n",
    "2. **Financial Concepts Involved:** (List specific topics like interest rates, inflation, investor sentiment, EPS, risk premium, etc.)\n",
    "3. **Reasoning Type Needed:** (e.g., Cause-effect, Comparison, Trend inference, Risk evaluation)\n",
    "4. **Potential Data Needed:** (e.g., Interest rate trend, Company earnings report, Market sentiment index...)\n",
    "5. **Difficulty Level:** (Basic / Intermediate / Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "Example:\n",
    "\n",
    "**Question:**  \n",
    "\"The central bank raised interest rates while inflation is slowing. What will happen to stock prices?\"\n",
    "\n",
    "**Response:**  \n",
    "1. **Main Topic:** Stock price direction  \n",
    "2. **Financial Concepts Involved:** Interest rates, Inflation, Equity valuation  \n",
    "3. **Reasoning Type Needed:** Cause-effect and macroeconomic trend analysis  \n",
    "4. **Potential Data Needed:** Fed rate decision, CPI trend, bond yield curve  \n",
    "5. **Difficulty Level:** Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "Now analyze this question:\n",
    "\n",
    "**Question:** {<<< YOUR QUESTION HERE >>>}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: Loop generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Qwen 3 8B ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö\n",
    "\n",
    "r = 0  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏±‡∏ö‡∏£‡∏≠‡∏ö\n",
    "ans = []  # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "for x, i in df.iterrows():  # loop ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å DataFrame\n",
    "    a = i['query']  # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
    "\n",
    "    # ‡∏£‡∏ß‡∏° query ‡πÅ‡∏•‡∏∞ concept ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\"}  # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏±‡∏ö concept\n",
    "    ]\n",
    "\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt template ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False  # ‚ùå ‡∏õ‡∏¥‡∏î thinking mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ‡πÉ‡∏´‡πâ model generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=200,  # ‚úÖ ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "        do_sample=False,  # ‚úÖ ‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏° ‡πÉ‡∏ä‡πâ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô warning ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á padding\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ token ‡∏ó‡∏µ‡πà model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á token ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('concept.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9798c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcc = pd.read_csv('/data/Nes/Finalhack/concept.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852dffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaa = pd.read_csv('/data/Nes/Finalhack/analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/data/week10/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"concept\"] = dfcc[\"concept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"analysis\"] = dfaa[\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87949c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt ‡∏ï‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly analytical and domain-aware expert across the fields of Ethical Finance, Marketing, Port Management, Finance Management, Economics, Stock Movement, Fixed Income, Accounting, and Financial Reporting.\n",
    "\n",
    "Your role is to answer expert-level questions from these domains using internal step-by-step reasoning (Chain of Thought), but you must return ONLY the final answer without explanation.\n",
    "\n",
    "Instructions:\n",
    "Think step-by-step internally to arrive at the best answer (Chain of Thought reasoning is allowed internally).\n",
    "If the question provides multiple choices (e.g., A, B, C, D, E, F...), select and return ONLY one letter that corresponds to the best choice.\n",
    "If the question is about stock price, market movement, or share performance, return ONLY one of the following:\n",
    "\"Rise\" (if price or value is expected to increase)\n",
    "\"Fall\" (if price or value is expected to decrease)\n",
    "Do NOT display any explanations, reasoning, or justifications in your response.\n",
    "The final output must always follow this format exactly:\n",
    "Answer: <A/B/C/.../Rise/Fall>\n",
    "\n",
    "Remember: The answer must appear alone on the last line, prefixed by \"Answer: \".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: Loop generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Qwen 3 8B ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö\n",
    "\n",
    "r = 0  # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏±‡∏ö‡∏£‡∏≠‡∏ö\n",
    "ans = []  # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "\n",
    "for x, i in df.iterrows():  # loop ‡∏ó‡∏µ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å DataFrame\n",
    "    a = i['query']  # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
    "    b = i['concept']  # ‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ã‡∏õ‡∏ï‡πå\n",
    "    c = i['analysis']  # ‡∏Ñ‡∏≥‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n",
    "\n",
    "    # ‡∏£‡∏ß‡∏° query ‡πÅ‡∏•‡∏∞ concept ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô user message\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},  # system prompt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {a}\\nConcept: {b}\\nAnalysis: {c}\"}  # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏±‡∏ö concept\n",
    "    ]\n",
    "\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt template ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Qwen\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True  # ‚ùå ‡∏õ‡∏¥‡∏î thinking mode ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏ï‡∏≠‡∏ö‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô input tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # ‡πÉ‡∏´‡πâ model generate ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=10000,  # ‚úÖ ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß output ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡∏õ‡∏¥‡∏î thinking ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô‡πÉ‡∏ä‡πâ 100 \n",
    "        do_sample=False,  # ‚úÖ ‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏° ‡πÉ‡∏ä‡πâ greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id  # ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô warning ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á padding\n",
    "    )\n",
    "\n",
    "    # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ token ‡∏ó‡∏µ‡πà model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° prompt)\n",
    "    new_token_ids = generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á token ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    response = tokenizer.decode(new_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "    print(\"content:\", response)\n",
    "    ans.append(response)\n",
    "    print(r)\n",
    "    r += 1  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8619bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = pd.read_csv(\"/data/data/week10/submission (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb[\"answer\"]  = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"Answer: \", \"\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"‡∏Ç‡∏∂‡πâ‡∏ô\", \"Rise\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(\"‡∏•‡∏á\", \"Fall\").str.strip()\n",
    "ans[\"answer\"] = ans[\"answer\"].str.replace(r\"\\.\\s*</think>\", \"\", regex=True).str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.to_csv(\"qwen38b+answer+concept.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc7aab",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36556e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = pd.read_csv(\"/data/Nes/Finalhack/qwen38b+answer+concept.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = pd.read_csv(\"/data/data/week10/processed_financial_analysis_flash2-5_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb058ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ch_preds = ch['answer']  # ‡πÄ‡∏õ‡πá‡∏ô Series ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• ch\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å sb ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô column \"answer\"\n",
    "sb_truth = sb['answer']  # ‡πÄ‡∏õ‡πá‡∏ô Series ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å ch ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á sb\n",
    "acc_ch = accuracy_score(sb_truth, ch_preds)  # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\n",
    "\n",
    "print(f\"Accuracy ‡∏Ç‡∏≠‡∏á ch: {acc_ch:.4f}\")  # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° 4 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # üßº ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏•‡∏∞‡∏•‡∏ö NaN\n",
    "# ch_preds = ch['answer'].astype(str)      # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string\n",
    "# sb_truth = ans['answer'].astype(str)     # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô string\n",
    "\n",
    "# # üßπ ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ missing\n",
    "# mask = (ch_preds.notna()) & (sb_truth.notna())\n",
    "# ch_preds_clean = ch_preds[mask]\n",
    "# sb_truth_clean = sb_truth[mask]\n",
    "\n",
    "# # ‚úÖ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "# acc_ch = accuracy_score(sb_truth_clean, ch_preds_clean)\n",
    "\n",
    "# # üñ®Ô∏è ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•\n",
    "# print(f\"Accuracy ‡∏Ç‡∏≠‡∏á ch: {acc_ch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.to_csv(\"qwen38b+answer+concept.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8b21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
